{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# train = pd.read_csv('data/train.csv', index_col='ex_id')\n",
    "# dev = pd.read_csv('data/dev.csv', index_col='ex_id')\n",
    "# test = pd.read_csv('data/test_no_label.csv', index_col='ex_id')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nlp = spacy.load('en_core_web_sm')\n",
    "# nlp.Defaults.stop_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(data):\n",
    "    \n",
    "    nlp = spacy.load('en_core_web_sm')\n",
    "    data_tokenized = []\n",
    "    \n",
    "    def tokenize_(review):\n",
    "        doc = nlp(review)\n",
    "        tokens = []\n",
    "        for tk in doc:\n",
    "            if tk.is_punct or tk.is_stop or tk.is_space:\n",
    "                continue # discard if is punctuation / stopword / whitespace\n",
    "            elif any([char.isdigit() for char in tk.text]):\n",
    "                continue # discard if is a number\n",
    "            else:\n",
    "                tokens.append(tk.lemma_.lower())\n",
    "        return tokens\n",
    "    \n",
    "    for review in data:\n",
    "        tokens = tokenize_(review)\n",
    "        data_tokenized.append(tokens)\n",
    "\n",
    "    return data_tokenized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vocab_set(data_tokenized):\n",
    "    return set.union(*[set(tokens) for tokens in data_tokenized])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle as pkl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tain\n",
    "train_data_tokens = tokenize(train.review.values)\n",
    "pkl.dump(train_data_tokens, open(\"train_data_tokens.pkl\", \"wb\"))\n",
    "\n",
    "# Val\n",
    "val_data_tokens = tokenize(dev.review.values)\n",
    "pkl.dump(val_data_tokens, open(\"val_data_tokens.pkl\", \"wb\"))\n",
    "\n",
    "# Test\n",
    "test_data_tokens = tokenize(test.review.values)\n",
    "pkl.dump(test_data_tokens, open(\"test_data_tokens.pkl\", \"wb\"))\n",
    "\n",
    "# Vocab\n",
    "all_train_tokens = get_vocab_set(train_data_tokens)\n",
    "pkl.dump(all_train_tokens, open(\"all_train_tokens.pkl\", \"wb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle as pkl\n",
    "from itertools import chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset size is 250874\n",
      "Val dataset size is 35918\n",
      "Test dataset size is 72165\n",
      "Total number of tokens in train dataset is 12969997\n",
      "Total number of *unique* tokens in train dataset is 114051\n"
     ]
    }
   ],
   "source": [
    "# Then, load preprocessed train, val and test datasets\n",
    "train_data_tokens = pkl.load(open(\"../data/tokens/train_data_tokens.pkl\", \"rb\"))\n",
    "val_data_tokens = pkl.load(open(\"../data/tokens/val_data_tokens.pkl\", \"rb\"))\n",
    "test_data_tokens = pkl.load(open(\"../data/tokens/test_data_tokens.pkl\", \"rb\"))\n",
    "\n",
    "# double checking\n",
    "print (\"Train dataset size is {}\".format(len(train_data_tokens)))\n",
    "print (\"Val dataset size is {}\".format(len(val_data_tokens)))\n",
    "print (\"Test dataset size is {}\".format(len(test_data_tokens)))\n",
    "\n",
    "\n",
    "all_train_tokens = list(chain.from_iterable(train_data_tokens))\n",
    "all_train_tokens_set = set(all_train_tokens)\n",
    "\n",
    "print (\"Total number of tokens in train dataset is {}\".format(len(all_train_tokens)))\n",
    "print (\"Total number of *unique* tokens in train dataset is {}\".format(len(set(all_train_tokens))))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
