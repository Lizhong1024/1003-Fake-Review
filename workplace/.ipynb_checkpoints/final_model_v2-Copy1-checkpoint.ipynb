{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle as pkl\n",
    "from itertools import chain\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "train = pd.read_csv('../data/train.csv', index_col='ex_id')\n",
    "val = pd.read_csv('../data/dev.csv', index_col='ex_id')\n",
    "test = pd.read_csv('../data/test_no_label.csv', index_col='ex_id')\n",
    "\n",
    "# Load tokenized data\n",
    "train_data_tokens = pkl.load(open(\"../data/tokens/train_data_tokens.pkl\", \"rb\"))\n",
    "val_data_tokens = pkl.load(open(\"../data/tokens/val_data_tokens.pkl\", \"rb\"))\n",
    "test_data_tokens = pkl.load(open(\"../data/tokens/test_data_tokens.pkl\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine train and val for final\n",
    "train = pd.concat([train, val])\n",
    "train_data_tokens = train_data_tokens + val_data_tokens\n",
    "all_train_tokens = list(chain.from_iterable(train_data_tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vocab\n",
    "def build_vocab(all_tokens, threshold):\n",
    "    c = Counter(all_tokens)\n",
    "    vocab = [word for word, count in Counter(all_train_tokens).items() if count >= threshold]\n",
    "    id2token = vocab\n",
    "    token2id = dict(zip(vocab, range(len(vocab))))\n",
    "    return token2id, id2token\n",
    "\n",
    "token2id, id2token = build_vocab(all_train_tokens, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TFIDF Vectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "def dummy(doc):\n",
    "    return doc\n",
    "\n",
    "tfidf_vec = TfidfVectorizer(lowercase=False, preprocessor=dummy, tokenizer=dummy, vocabulary=token2id)  \n",
    "\n",
    "X_train_tfidf = tfidf_vec.fit_transform(train_data_tokens)\n",
    "X_test_tfidf = tfidf_vec.transform(test_data_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get labels\n",
    "y_train = train.label.values\n",
    "y_test = test.label.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Latent Factor Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### MyAlgo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from surprise import AlgoBase\n",
    "from surprise import SVD\n",
    "from surprise import PredictionImpossible\n",
    "\n",
    "class MyAlgo(SVD):\n",
    "\n",
    "    def __init__(self, n_factors=25, n_epochs=20, biased=False, \n",
    "                 lr_all=.005, reg_all=.1, random_state=None, verbose=False):\n",
    "\n",
    "        SVD.__init__(self, n_factors=n_factors, n_epochs=n_epochs, \n",
    "                     biased=biased, lr_all=lr_all, reg_all=reg_all, \n",
    "                     random_state=random_state, verbose=verbose)\n",
    "\n",
    "    def fit(self, trainset):\n",
    "\n",
    "        SVD.fit(self, trainset)\n",
    "\n",
    "        return self\n",
    "\n",
    "    def estimate(self, u, i):\n",
    "\n",
    "        known_user = self.trainset.knows_user(u)\n",
    "        known_item = self.trainset.knows_item(i)\n",
    "\n",
    "        if known_user and known_item:\n",
    "\n",
    "            if self.biased:\n",
    "                est = self.trainset.global_mean\n",
    "                if known_user:\n",
    "                    est += self.bu[u]\n",
    "\n",
    "                if known_item:\n",
    "                    est += self.bi[i]\n",
    "\n",
    "                if known_user and known_item:\n",
    "                    est += np.dot(self.qi[i], self.pu[u])\n",
    "\n",
    "            else:\n",
    "                est = np.dot(self.qi[i], self.pu[u])    \n",
    "\n",
    "        else:\n",
    "            est = 0\n",
    "            raise PredictionImpossible('User and item are unknown.')\n",
    "\n",
    "        return est\n",
    "    \n",
    "    def test(self, testset, clip=False, verbose=False):\n",
    "        predictions = [self.predict(uid,\n",
    "                                    iid,\n",
    "                                    r_ui_trans,\n",
    "                                    clip=clip,\n",
    "                                    verbose=verbose)\n",
    "                       for (uid, iid, r_ui_trans) in testset]\n",
    "        return predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Construct ALS Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from surprise import Dataset, Reader, accuracy\n",
    "\n",
    "# Get train data\n",
    "# SELECT user_id, prod_id, rating FROM train WHERE label = 0\n",
    "train_als = train[(train['label'] == 0)][['user_id', 'prod_id', 'rating']]\n",
    "\n",
    "# The columns must correspond to user id, item id and ratings (in that order)\n",
    "reader = Reader(rating_scale=(1, 5))\n",
    "data = Dataset.load_from_df(train_als[['user_id', 'prod_id', 'rating']], reader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.MyAlgo at 0x127a07b50>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rank = 25\n",
    "reg = 0.1\n",
    "\n",
    "algo = MyAlgo(n_factors=rank, reg_all=reg, biased=True,\n",
    "              lr_all=0.005, n_epochs=30, verbose=False, random_state=None)\n",
    "\n",
    "algo.fit(data.build_full_trainset())\n",
    "\n",
    "# user_factors = algo.pu\n",
    "# prod_factors = algo.qi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generate rating features & Combine rating features and review features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.sparse import csr_matrix, coo_matrix, hstack\n",
    "\n",
    "def combine_features(user_features, rating_features, review_features):\n",
    "    return hstack( [user_features, csr_matrix(rating_features), review_features], format='csr' )\n",
    "\n",
    "def get_rating_features(val, algo):\n",
    "    testset = list(zip(val['user_id'].values, val['prod_id'].values, val['rating'].values))\n",
    "    predictions = algo.test(testset)\n",
    "    pred_rating = []\n",
    "    is_missing = []\n",
    "    actual_rating = []\n",
    "    diff = []\n",
    "    \n",
    "    for pred in predictions:\n",
    "        pred_rating.append(pred.est)\n",
    "        is_missing.append( int(pred.details['was_impossible']) )\n",
    "        actual_rating.append(pred.r_ui)\n",
    "        diff.append(pred.r_ui - pred.est)\n",
    "    \n",
    "    rating_features = list(zip(pred_rating, is_missing, actual_rating, diff))\n",
    "\n",
    "    return rating_features\n",
    "\n",
    "def get_user_features(dataset):\n",
    "    users = dataset['user_id']\n",
    "    users_fake_cnts = Counter(train[train['label'] == 1]['user_id'])\n",
    "    users_feature = [users_fake_cnts[user] if user in users_fake_cnts else 0 for user in users]\n",
    "    return np.array(users_feature).reshape([len(users_feature), 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_rating_features = get_rating_features(train, algo)\n",
    "test_rating_features = get_rating_features(test, algo)\n",
    "\n",
    "train_users_features = get_user_features(train)\n",
    "test_users_features = get_user_features(test)\n",
    "\n",
    "X_train = combine_features(train_users_features, train_rating_features, X_train_tfidf)\n",
    "X_test = combine_features(test_users_features, test_rating_features, X_test_tfidf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=0.4, class_weight=None, dual=False, fit_intercept=True,\n",
       "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
       "                   multi_class='auto', n_jobs=None, penalty='l2',\n",
       "                   random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                   warm_start=False)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr = LogisticRegression(C=0.4, max_iter=1000)\n",
    "lr.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = lr.predict_proba(X_test)[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict\n",
    "test['pred'] = pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save result\n",
    "test[['pred']].to_csv('predictions_v2.csv', index=False, header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>prod_id</th>\n",
       "      <th>rating</th>\n",
       "      <th>label</th>\n",
       "      <th>date</th>\n",
       "      <th>review</th>\n",
       "      <th>pred</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ex_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>929</td>\n",
       "      <td>0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2009-08-25</td>\n",
       "      <td>Let me start with a shout-out to everyone who ...</td>\n",
       "      <td>0.997499</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>932</td>\n",
       "      <td>0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2014-05-09</td>\n",
       "      <td>Stopped in for lunch today and couldn't believ...</td>\n",
       "      <td>0.998199</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>937</td>\n",
       "      <td>0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2014-10-15</td>\n",
       "      <td>Tiny little place, but very good food. Pastits...</td>\n",
       "      <td>0.002167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>945</td>\n",
       "      <td>0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2014-04-10</td>\n",
       "      <td>Food was delicious and service was great. Good...</td>\n",
       "      <td>0.998846</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>946</td>\n",
       "      <td>0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2014-03-29</td>\n",
       "      <td>Awesome hole in the wall place to grab a quick...</td>\n",
       "      <td>0.001729</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>358923</th>\n",
       "      <td>3429</td>\n",
       "      <td>349</td>\n",
       "      <td>3.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2014-04-08</td>\n",
       "      <td>Meh.  I guess this might have been a little ov...</td>\n",
       "      <td>0.001542</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>358924</th>\n",
       "      <td>161137</td>\n",
       "      <td>349</td>\n",
       "      <td>5.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2014-04-05</td>\n",
       "      <td>Thank god we got in when we did. There was but...</td>\n",
       "      <td>0.997644</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>358936</th>\n",
       "      <td>41730</td>\n",
       "      <td>349</td>\n",
       "      <td>5.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2014-03-03</td>\n",
       "      <td>when tasting table sent me an email about this...</td>\n",
       "      <td>0.003661</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>358952</th>\n",
       "      <td>161146</td>\n",
       "      <td>349</td>\n",
       "      <td>5.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2014-02-06</td>\n",
       "      <td>I'm very spoiled with Pizza. Really, I have tr...</td>\n",
       "      <td>0.998315</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>358955</th>\n",
       "      <td>97930</td>\n",
       "      <td>349</td>\n",
       "      <td>5.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2014-01-25</td>\n",
       "      <td>Great foods and great drinks, they have even p...</td>\n",
       "      <td>0.006312</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>72165 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        user_id  prod_id  rating  label        date  \\\n",
       "ex_id                                                 \n",
       "6           929        0     4.0    NaN  2009-08-25   \n",
       "9           932        0     5.0    NaN  2014-05-09   \n",
       "14          937        0     4.0    NaN  2014-10-15   \n",
       "22          945        0     5.0    NaN  2014-04-10   \n",
       "23          946        0     5.0    NaN  2014-03-29   \n",
       "...         ...      ...     ...    ...         ...   \n",
       "358923     3429      349     3.0    NaN  2014-04-08   \n",
       "358924   161137      349     5.0    NaN  2014-04-05   \n",
       "358936    41730      349     5.0    NaN  2014-03-03   \n",
       "358952   161146      349     5.0    NaN  2014-02-06   \n",
       "358955    97930      349     5.0    NaN  2014-01-25   \n",
       "\n",
       "                                                   review      pred  \n",
       "ex_id                                                                \n",
       "6       Let me start with a shout-out to everyone who ...  0.997499  \n",
       "9       Stopped in for lunch today and couldn't believ...  0.998199  \n",
       "14      Tiny little place, but very good food. Pastits...  0.002167  \n",
       "22      Food was delicious and service was great. Good...  0.998846  \n",
       "23      Awesome hole in the wall place to grab a quick...  0.001729  \n",
       "...                                                   ...       ...  \n",
       "358923  Meh.  I guess this might have been a little ov...  0.001542  \n",
       "358924  Thank god we got in when we did. There was but...  0.997644  \n",
       "358936  when tasting table sent me an email about this...  0.003661  \n",
       "358952  I'm very spoiled with Pizza. Really, I have tr...  0.998315  \n",
       "358955  Great foods and great drinks, they have even p...  0.006312  \n",
       "\n",
       "[72165 rows x 7 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
