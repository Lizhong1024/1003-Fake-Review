{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle as pkl\n",
    "from itertools import chain\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "train = pd.read_csv('../data/train.csv', index_col='ex_id')\n",
    "val = pd.read_csv('../data/dev.csv', index_col='ex_id')\n",
    "test = pd.read_csv('../data/test_no_label.csv', index_col='ex_id')\n",
    "\n",
    "# Load tokenized data\n",
    "train_data_tokens = pkl.load(open(\"../data/tokens/train_data_tokens.pkl\", \"rb\"))\n",
    "val_data_tokens = pkl.load(open(\"../data/tokens/val_data_tokens.pkl\", \"rb\"))\n",
    "test_data_tokens = pkl.load(open(\"../data/tokens/test_data_tokens.pkl\", \"rb\"))\n",
    "all_train_tokens = list(chain.from_iterable(train_data_tokens))\n",
    "\n",
    "# Get labels\n",
    "y_train = train.label.values\n",
    "y_val = val.label.values\n",
    "y_test = test.label.values\n",
    "\n",
    "# Vocab\n",
    "def build_vocab(all_tokens, threshold):\n",
    "    c = Counter(all_tokens)\n",
    "    vocab = [word for word, count in Counter(all_train_tokens).items() if count >= threshold]\n",
    "    id2token = vocab\n",
    "    token2id = dict(zip(vocab, range(len(vocab))))\n",
    "    return token2id, id2token\n",
    "\n",
    "token2id, id2token = build_vocab(all_train_tokens, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TFIDF Vectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "def dummy(doc):\n",
    "    return doc\n",
    "\n",
    "tfidf_vec = TfidfVectorizer(lowercase=False, preprocessor=dummy, tokenizer=dummy, vocabulary=token2id)  \n",
    "\n",
    "X_train_tfidf = tfidf_vec.fit_transform(train_data_tokens)\n",
    "X_val_tfidf = tfidf_vec.transform(val_data_tokens)\n",
    "X_test_tfidf = tfidf_vec.transform(test_data_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train_tfidf\n",
    "X_val = X_val_tfidf\n",
    "X_test = X_test_tfidf\n",
    "\n",
    "y_train = train.label.values\n",
    "y_val = val.label.values\n",
    "y_test = test.label.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score, classification_report\n",
    "from sklearn.metrics import roc_curve, auc, confusion_matrix\n",
    "from sklearn.metrics import average_precision_score\n",
    "\n",
    "def evaluate(model, X=X_val_tfidf, y=y_val):\n",
    "    y_scores = model.predict_proba(X)[:, 1]\n",
    "    print('Accuracy: ', model.score(X, y))\n",
    "    print('AUC: ', roc_auc_score(y, y_scores))\n",
    "    print('AP: ', average_precision_score(y, y_scores))\n",
    "    print('\\nConfusion Matrix')\n",
    "    print(confusion_matrix(y, model.predict(X)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=0.4, class_weight=None, dual=False, fit_intercept=True,\n",
       "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
       "                   multi_class='auto', n_jobs=None, penalty='l2',\n",
       "                   random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                   warm_start=False)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr_0 = LogisticRegression(C=0.4, max_iter=1000)\n",
    "lr_0.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.8981290717745977\n",
      "AUC:  0.7236413209678209\n",
      "AP:  0.2137571667354642\n",
      "\n",
      "Confusion Matrix\n",
      "[[32250    20]\n",
      " [ 3639     9]]\n"
     ]
    }
   ],
   "source": [
    "evaluate(lr_0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Downsample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "rus = RandomUnderSampler()\n",
    "X_train_rus, y_train_rus = rus.fit_resample(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=0.4, class_weight=None, dual=False, fit_intercept=True,\n",
       "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
       "                   multi_class='auto', n_jobs=None, penalty='l2',\n",
       "                   random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                   warm_start=False)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr_rus = LogisticRegression(C=0.4, max_iter=1000)\n",
    "lr_rus.fit(X_train_rus, y_train_rus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.651567459212651\n",
      "AUC:  0.7174633727077999\n",
      "AP:  0.20851932878214086\n",
      "\n",
      "Confusion Matrix\n",
      "[[20974 11296]\n",
      " [ 1219  2429]]\n"
     ]
    }
   ],
   "source": [
    "evaluate(lr_rus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NearMiss 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.under_sampling import NearMiss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "nm1 = NearMiss(version=1)\n",
    "X_train_nm1, y_train_nm1 = nm1.fit_resample(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=0.4, class_weight=None, dual=False, fit_intercept=True,\n",
       "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
       "                   multi_class='auto', n_jobs=None, penalty='l2',\n",
       "                   random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                   warm_start=False)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr_nm1 = LogisticRegression(C=0.4, max_iter=1000)\n",
    "lr_nm1.fit(X_train_nm1, y_train_nm1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.5047051617573362\n",
      "AUC:  0.6352713569444217\n",
      "AP:  0.1619861041238025\n",
      "\n",
      "Confusion Matrix\n",
      "[[15562 16708]\n",
      " [ 1082  2566]]\n"
     ]
    }
   ],
   "source": [
    "evaluate(lr_nm1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NearMiss 2 (这个跑不出来...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "跑了好几次  每次kernel都死掉"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nm2 = NearMiss(version=2)\n",
    "X_train_nm2, y_train_nm2 = nm2.fit_resample(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_nm2 = LogisticRegression(C=0.4, max_iter=1000)\n",
    "lr_nm2.fit(X_train_nm2, y_train_nm2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate(lr_nm2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NearMiss 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/lizhong/anaconda3/lib/python3.7/site-packages/imblearn/under_sampling/_prototype_selection/_nearmiss.py:175: UserWarning: The number of the samples to be selected is larger than the number of samples available. The balancing ratio cannot be ensure and all samples will be returned.\n",
      "  \"The number of the samples to be selected is larger\"\n"
     ]
    }
   ],
   "source": [
    "nm3 = NearMiss(version=3)\n",
    "X_train_nm3, y_train_nm3 = nm3.fit_resample(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=0.4, class_weight=None, dual=False, fit_intercept=True,\n",
       "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
       "                   multi_class='auto', n_jobs=None, penalty='l2',\n",
       "                   random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                   warm_start=False)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr_nm3 = LogisticRegression(C=0.4, max_iter=1000)\n",
    "lr_nm3.fit(X_train_nm3, y_train_nm3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.10596358371846985\n",
      "AUC:  0.4517689543136583\n",
      "AP:  0.09337850963938217\n",
      "\n",
      "Confusion Matrix\n",
      "[[  212 32058]\n",
      " [   54  3594]]\n"
     ]
    }
   ],
   "source": [
    "evaluate(lr_nm3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
